{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9b8b58a4",
      "metadata": {
        "id": "9b8b58a4"
      },
      "source": [
        "# Q1. ax, Autograd and Neural Networks (40 points + 14 Bonus points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0xhC8Tt3Ttai",
      "metadata": {
        "id": "0xhC8Tt3Ttai"
      },
      "source": [
        "## (a) Univariate Function Gradient (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "724948d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "724948d1",
        "outputId": "11e278ba-16aa-4bde-e1f6-02ad15bf15e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient: 22.353619 22.353619\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "import numpy as np\n",
        "\n",
        "# Q1a.  Example for taking the derivative of a simple function\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.cos(x**3 - 2*x**2 + 5)\n",
        "\n",
        "def manual_grad(x):\n",
        "    # Implement the derivative w.r.t. x using the chain rule\n",
        "    # f(x) = cos(u(x)) where u(x) = x^3 - 2*x^2 + 5\n",
        "    # f'(x) = -sin(u(x)) * u'(x)\n",
        "    u = x**3 - 2*x**2 + 5\n",
        "    du_dx = 3*x**2 - 4*x\n",
        "    gradient = -jnp.sin(u) * du_dx\n",
        "    return gradient\n",
        "\n",
        "grad_fun = grad(fun)\n",
        "\n",
        "x = 3.5\n",
        "\n",
        "print(\"Gradient:\", grad_fun(x),  manual_grad(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jwTLgYvTUAU2",
      "metadata": {
        "id": "jwTLgYvTUAU2"
      },
      "source": [
        "## (b) Multivariate Function Gradient (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bc522d1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc522d1e",
        "outputId": "3c914b69-bf24-48f8-ba82-c6404e37dac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient: [80.902725  4.854164 32.36109 ] [80.90272   4.854163 32.361088]\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "def cubic_l2_norm(x):\n",
        "    return jnp.linalg.norm(x)**3\n",
        "\n",
        "# Automatic differentiation gradient using JAX\n",
        "grad_fun = grad(cubic_l2_norm)\n",
        "\n",
        "x = jnp.array([5.0, 0.3, 2.0])\n",
        "\n",
        "def manual_grad(x):\n",
        "    # Compute the L2 norm\n",
        "    r = jnp.linalg.norm(x)\n",
        "    # Applying the chain rule:\n",
        "    # f(x) = (r)^3, so f'(r) = 3 * r^2 and âˆ‡r = x / r.\n",
        "    # Thus, gradient = 3 * r^2 * (x / r) = 3 * r * x.\n",
        "    gradient = 3 * r * x\n",
        "    return gradient\n",
        "\n",
        "print(\"Gradient:\", grad_fun(x), manual_grad(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zTVA8MxlcED5",
      "metadata": {
        "id": "zTVA8MxlcED5"
      },
      "source": [
        "### Implement a function that computes the Hessian matrix for the given function (2 Bonus points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "def cubic_l2_norm(x):\n",
        "    return jnp.linalg.norm(x)**3\n",
        "\n",
        "def hessian_manual(f, x):\n",
        "    n = x.shape[0]\n",
        "    hess = jnp.zeros((n, n))\n",
        "\n",
        "    for i in range(n):\n",
        "        grad_i = grad(f, argnums=0)\n",
        "        def partial_derivative(x):\n",
        "            return grad_i(x)[i]\n",
        "\n",
        "        hess_i = grad(partial_derivative, argnums=0)(x)\n",
        "        hess = hess.at[i].set(hess_i)\n",
        "\n",
        "    return hess\n",
        "\n",
        "x = jnp.array([5.0, 0.3, 2.0])\n",
        "hessian_matrix = hessian_manual(cubic_l2_norm, x)\n",
        "print(\"Hessian Matrix:\", hessian_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjm9W6bUt9qO",
        "outputId": "bab05b02-f022-4124-f580-02010c06f7ca"
      },
      "id": "gjm9W6bUt9qO",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hessian Matrix: [[30.086134    0.8343353   5.5622354 ]\n",
            " [ 0.8343354  16.230606    0.33373415]\n",
            " [ 5.562236    0.33373415 18.40544   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uWh6DxzsUNf-",
      "metadata": {
        "id": "uWh6DxzsUNf-"
      },
      "source": [
        "## (c) Soft-Max Regression and Neural Networks (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "31d7816a",
      "metadata": {
        "id": "31d7816a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Q1c. Neural Network from Scratch\n",
        "# Example implementation of simple soft-max regression with square loss\n",
        "#[Nothing to implement in this block]\n",
        "\n",
        "# define soft argmax\n",
        "def soft_argmax(x):\n",
        "    y = jnp.exp(x)\n",
        "    return y/jnp.sum(y)\n",
        "\n",
        "# Specify the how to caculate the\n",
        "\n",
        "def predict(params, inputs):\n",
        "    W = params[0]\n",
        "    b = params[1]\n",
        "    scores = jnp.dot( W, inputs) + b\n",
        "    outputs = soft_argmax(scores)\n",
        "    return outputs\n",
        "\n",
        "def loss_fun(params, inputs, targets):\n",
        "    preds = predict(params, inputs)\n",
        "    # You can use cross-entropy loss instead\n",
        "    return jnp.sum((preds - targets)**2)\n",
        "\n",
        "grad_fun = grad(loss_fun)  # gradient evaluation function\n",
        "\n",
        "# Lets' cook up some input for this\n",
        "W = np.random.randn(3,4)\n",
        "b = np.random.randn(3,1)\n",
        "inputs = np.random.randn(4,1)\n",
        "targets = np.array([1,0,0])\n",
        "params=[W,b]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "28004cae",
      "metadata": {
        "id": "28004cae",
        "outputId": "05c04bc9-2282-4eeb-cba4-5316b07d64b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7718097]\n",
            " [0.023276 ]\n",
            " [0.2049143]]\n",
            "[Array([[ 0.44290727, -1.1176932 ,  0.34664053, -0.39820448],\n",
            "       [-0.06148668,  0.15516396, -0.04812244,  0.05528081],\n",
            "       [-0.3814207 ,  0.9625295 , -0.29851818,  0.34292376]],      dtype=float32), Array([[ 0.61862624],\n",
            "       [-0.0858809 ],\n",
            "       [-0.5327455 ]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "# Example usage of above functions\n",
        "# [Nothing to implement in this block]\n",
        "\n",
        "# How to Generate a prediction for this inputs\n",
        "print(predict(params, inputs))\n",
        "\n",
        "# How to Compute the gradient from this imput\n",
        "print(grad_fun(params, inputs,targets))\n",
        "\n",
        "# How to implement an SGD udpate\n",
        "# Take a minibatch of data X,y\n",
        "def sgd_update(params,X,y):\n",
        "    lr = 0.01\n",
        "    gradient = grad_fun(params,X,y)\n",
        "    for param, g in zip(params,gradients):\n",
        "        param -= lr*g\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e2b68f",
      "metadata": {
        "id": "a4e2b68f"
      },
      "source": [
        "**Now you are expected to implement a two-layer neural network from scratch by modifying the above code for soft-max regression.**\n",
        "\n",
        "**Very Important the only function you need to modify is** `predict_nn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "50e207d7",
      "metadata": {
        "id": "50e207d7"
      },
      "outputs": [],
      "source": [
        "# Q1c.  continues\n",
        "\n",
        "#  You may use jnp.tanh to implement a hyperbolic tangent activation fuonction.\n",
        "#\n",
        "\n",
        "def soft_argmax(x):\n",
        "    y = jnp.exp(x)\n",
        "    return y/jnp.sum(y)\n",
        "\n",
        "def predict_nn(params, inputs):\n",
        "    W1 = params[0]\n",
        "    b1 = params[1]\n",
        "    W2 = params[2]\n",
        "    b2 = params[3]\n",
        "\n",
        "    # First layer: linear transformation then tanh activation\n",
        "    hidden = jnp.tanh(jnp.dot(W1, inputs) + b1)\n",
        "    # Second layer: linear transformation to obtain scores\n",
        "    scores = jnp.dot(W2, hidden) + b2\n",
        "    outputs = soft_argmax(scores)\n",
        "    return outputs\n",
        "\n",
        "def loss_fun_nn(params, inputs, targets):\n",
        "    preds = predict_nn(params, inputs)\n",
        "    # Using squared error loss\n",
        "    return jnp.sum((preds - targets)**2)\n",
        "\n",
        "\n",
        "grad_fun_nn = grad(loss_fun_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HQhTYtcKUy4x",
      "metadata": {
        "id": "HQhTYtcKUy4x"
      },
      "source": [
        "## (d) Neural Network Parameters and Gradient (15 points)\n",
        "Now let's say the neural network is supposed to classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c68a53fd",
      "metadata": {
        "id": "c68a53fd"
      },
      "outputs": [],
      "source": [
        "# Q1d Lets' cook up some input:\n",
        "\n",
        "inputs = np.random.randn(64,1)\n",
        "targets = np.array([1,0,0])\n",
        "\n",
        "# What could be a valid shape of the following parameters?  Replace the question marks with valid numbers\n",
        "\n",
        "W1 = jnp.array(np.random.randn(6, 64), dtype=jnp.float32)\n",
        "b1 = jnp.array(np.random.randn(6, 1), dtype=jnp.float32)\n",
        "W2 = jnp.array(np.random.randn(3, 6), dtype=jnp.float32)\n",
        "b2 = jnp.array(np.random.randn(3, 1), dtype=jnp.float32)\n",
        "\n",
        "\n",
        "params=[W1,b1,W2,b2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "54bd2657",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54bd2657",
        "outputId": "dae94a4e-5a1f-418c-92d4-491461124d19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Array([[ 2.3542825e-06, -3.6968760e-07, -4.9402229e-06, -4.1394696e-06,\n",
              "         -9.8418186e-06,  8.0379522e-07,  5.4087349e-07,  4.8220763e-06,\n",
              "          4.6904765e-06,  3.5718469e-06, -3.4993764e-06, -3.2033065e-06,\n",
              "          1.7704331e-06,  1.7595565e-06, -1.9591282e-06, -3.9403026e-06,\n",
              "         -6.5035924e-06,  1.3518017e-07, -9.2539295e-07, -1.8957248e-07,\n",
              "          7.4537111e-06,  6.5348318e-06,  6.9858943e-06, -2.0507632e-06,\n",
              "          1.0439821e-06,  2.4046362e-06, -5.5359474e-06, -2.8329914e-06,\n",
              "          5.0868121e-06,  2.3051925e-06,  6.4688902e-07,  4.9020241e-07,\n",
              "         -4.4519857e-06,  3.5559385e-06,  1.5751663e-06, -2.8684735e-06,\n",
              "         -4.5185211e-06, -6.9223955e-07, -4.1694479e-06, -5.4795537e-06,\n",
              "         -1.8865945e-06, -2.5612119e-06, -3.6040640e-06,  2.8282750e-06,\n",
              "         -3.6670867e-06,  1.1330413e-06,  2.2795441e-06, -1.2136221e-06,\n",
              "         -7.9058336e-06,  6.0834745e-06, -5.1512361e-06,  4.0811137e-06,\n",
              "          1.2462170e-06,  2.4620606e-06,  5.6822437e-06,  1.7963852e-06,\n",
              "         -4.1749573e-07,  5.2931341e-07, -1.0182634e-07, -5.2803498e-06,\n",
              "         -1.2031593e-06, -5.6854219e-07, -2.1043290e-06,  5.1848583e-06],\n",
              "        [ 1.3752646e-03, -2.1595467e-04, -2.8858534e-03, -2.4180897e-03,\n",
              "         -5.7491427e-03,  4.6954060e-04,  3.1595369e-04,  2.8168375e-03,\n",
              "          2.7399631e-03,  2.0865104e-03, -2.0441764e-03, -1.8712259e-03,\n",
              "          1.0342065e-03,  1.0278529e-03, -1.1444335e-03, -2.3017456e-03,\n",
              "         -3.7991025e-03,  7.8966099e-05, -5.4057245e-04, -1.1073962e-04,\n",
              "          4.3541188e-03,  3.8173513e-03,  4.0808418e-03, -1.1979626e-03,\n",
              "          6.0984684e-04,  1.4046790e-03, -3.2338486e-03, -1.6549046e-03,\n",
              "          2.9714841e-03,  1.3465886e-03,  3.7788312e-04,  2.8635396e-04,\n",
              "         -2.6006473e-03,  2.0772174e-03,  9.2014042e-04, -1.6756317e-03,\n",
              "         -2.6395142e-03, -4.0437482e-04, -2.4356018e-03, -3.2009059e-03,\n",
              "         -1.1020626e-03, -1.4961434e-03, -2.1053301e-03,  1.6521495e-03,\n",
              "         -2.1421451e-03,  6.6187116e-04,  1.3316058e-03, -7.0894283e-04,\n",
              "         -4.6182279e-03,  3.5536890e-03, -3.0091179e-03,  2.3840009e-03,\n",
              "          7.2798325e-04,  1.4382238e-03,  3.3193082e-03,  1.0493664e-03,\n",
              "         -2.4388199e-04,  3.0920081e-04, -5.9482310e-05, -3.0845399e-03,\n",
              "         -7.0283090e-04, -3.3211650e-04, -1.2292532e-03,  3.0287583e-03],\n",
              "        [-1.8846178e-05,  2.9593725e-06,  3.9546794e-05,  3.3136712e-05,\n",
              "          7.8784382e-05, -6.4344313e-06, -4.3297264e-06, -3.8601025e-05,\n",
              "         -3.7547561e-05, -2.8592860e-05,  2.8012728e-05,  2.5642670e-05,\n",
              "         -1.4172429e-05, -1.4085361e-05,  1.5682945e-05,  3.1542371e-05,\n",
              "          5.2061663e-05, -1.0821258e-06,  7.4078289e-06,  1.5175397e-06,\n",
              "         -5.9667425e-05, -5.2311738e-05, -5.5922523e-05,  1.6416489e-05,\n",
              "         -8.3571422e-06, -1.9249264e-05,  4.4315610e-05,  2.2678274e-05,\n",
              "         -4.0720250e-05, -1.8453211e-05, -5.1783873e-06, -3.9241013e-06,\n",
              "          3.5638423e-05, -2.8465511e-05, -1.2609305e-05,  2.2962311e-05,\n",
              "          3.6171044e-05,  5.5414207e-06,  3.3376691e-05,  4.3864173e-05,\n",
              "          1.5102308e-05,  2.0502661e-05,  2.8850758e-05, -2.2640517e-05,\n",
              "          2.9355258e-05, -9.0700669e-06, -1.8247893e-05,  9.7151215e-06,\n",
              "          6.3286694e-05, -4.8698595e-05,  4.1235970e-05, -3.2669570e-05,\n",
              "         -9.9760455e-06, -1.9708950e-05, -4.5486719e-05, -1.4380175e-05,\n",
              "          3.3420795e-06, -4.2371871e-06,  8.1512621e-07,  4.2269530e-05,\n",
              "          9.6313661e-06,  4.5512161e-06,  1.6845286e-05, -4.1505114e-05],\n",
              "        [ 0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
              "        [ 0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
              "        [ 0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
              "         -0.0000000e+00,  0.0000000e+00, -0.0000000e+00, -0.0000000e+00,\n",
              "         -0.0000000e+00, -0.0000000e+00, -0.0000000e+00,  0.0000000e+00]],      dtype=float32),\n",
              " Array([[ 3.7337195e-06],\n",
              "        [ 2.1810690e-03],\n",
              "        [-2.9888659e-05],\n",
              "        [ 0.0000000e+00],\n",
              "        [ 0.0000000e+00],\n",
              "        [ 0.0000000e+00]], dtype=float32),\n",
              " Array([[ 0.35197467, -0.35068947,  0.35194796,  0.35197613, -0.35197613,\n",
              "         -0.35197613],\n",
              "        [-0.19344446,  0.1927381 , -0.19342977, -0.19344525,  0.19344525,\n",
              "          0.19344525],\n",
              "        [-0.15853024,  0.15795137, -0.1585182 , -0.15853089,  0.15853089,\n",
              "          0.15853089]], dtype=float32),\n",
              " Array([[ 0.35197613],\n",
              "        [-0.19344525],\n",
              "        [-0.15853089]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "predict_nn(params, inputs)\n",
        "\n",
        "grad_fun_nn(params, inputs,targets)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}